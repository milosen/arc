{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Controlled Stream Generation\n",
    "\n",
    "We will generate words and a lexicon with minimal feature overlap between words. Next, we introduce the 3 main ways to generate random streams based on a lexicon. Each specifies how the transition probabilities (TPs) of their syllables are structured:\n",
    "\n",
    "1. uniformlly distributed TPs, called \"TP-random position-random\" in the paper, \n",
    "2. position-controlled TPs, called \"TP-random position-fixed\", and\n",
    "3. TPs that fully preserve the words, called \"TP-structured\".\n",
    "\n",
    "## Installation\n",
    "\n",
    "> ⚠️ We recommend using a virtual environment\n",
    "\n",
    "> ⚠️ If you use a virtual environment, make sure you use the right kernel for this notebook. You can usually select it in the top right corner. If your environment is not in the list, you have to add the ipython kernel from the environment like so:\n",
    "> 1. Activate virtual environment in terminal\n",
    "> 2. Run `pip install ipykernel`\n",
    "> 3. Run `python -m ipykernel install --user --name arc --display-name \"Python (ARC)\"`\n",
    "> 4. Reload this page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d22326",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/milosen/arc.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e0ed36c",
   "metadata": {},
   "source": [
    "\n",
    "## Syllable and Word Generation\n",
    "\n",
    "Because ARC runs probabilistically (to speed things up), we set the random seeds to make sure our runs are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1953ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɡ|k|b|d|p|t|x|ç|ʃ|f|... (38 elements total)\n",
      "ɡaː|ɡiː|ɡyː|ɡɛː|kaː|koː|kuː|køː|kɛː|baː|... (75 elements total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 835.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram control...\n",
      "trigram control...\n",
      "positional control...\n",
      "çaːbøːriː|buːsiːheː|ʃiːmoːɡaː|boːhøːsaː|tuːhøːvaː|zuːpeːhoː|nøːfoːɡaː|saːpuːhøː|bøːzyːhuː|roːɡɛːfyː|... (10000 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc import load_phonemes, make_syllables, make_words, make_lexicons, make_streams\n",
    "\n",
    "phonemes = load_phonemes()\n",
    "print(phonemes)\n",
    "\n",
    "syllables = make_syllables(phonemes)\n",
    "print(syllables)\n",
    "\n",
    "words = make_words(syllables)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56bc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.save(\"test_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8873e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "import os\n",
    "\n",
    "webbrowser.open('file://' + os.path.realpath(\"test_words.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0153360a",
   "metadata": {},
   "source": [
    "## Get Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a64fac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/nikola/workspace/arc\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (2.0.0)\n",
      "Requirement already satisfied: scipy in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (4.66.4)\n",
      "Requirement already satisfied: pydantic in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (2.8.2)\n",
      "Requirement already satisfied: pytest in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (8.2.2)\n",
      "Requirement already satisfied: pingouin in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (0.5.4)\n",
      "Requirement already satisfied: matplotlib in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (3.9.1)\n",
      "Requirement already satisfied: Click in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from arc==1.0) (8.1.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from matplotlib->arc==1.0) (6.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pandas->arc==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pandas->arc==1.0) (2024.1)\n",
      "Requirement already satisfied: seaborn in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pingouin->arc==1.0) (0.13.2)\n",
      "Requirement already satisfied: statsmodels in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pingouin->arc==1.0) (0.14.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pingouin->arc==1.0) (1.5.1)\n",
      "Requirement already satisfied: pandas-flavor in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pingouin->arc==1.0) (0.6.0)\n",
      "Requirement already satisfied: tabulate in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pingouin->arc==1.0) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pydantic->arc==1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pydantic->arc==1.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pydantic->arc==1.0) (4.12.2)\n",
      "Requirement already satisfied: iniconfig in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pytest->arc==1.0) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.5 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pytest->arc==1.0) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pytest->arc==1.0) (1.2.1)\n",
      "Requirement already satisfied: tomli>=1 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pytest->arc==1.0) (2.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->arc==1.0) (3.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->arc==1.0) (1.16.0)\n",
      "Requirement already satisfied: xarray in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from pandas-flavor->pingouin->arc==1.0) (2024.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from scikit-learn->pingouin->arc==1.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from scikit-learn->pingouin->arc==1.0) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/nikola/miniforge3/envs/arc_workshop/lib/python3.9/site-packages (from statsmodels->pingouin->arc==1.0) (0.5.6)\n",
      "Installing collected packages: arc\n",
      "  Attempting uninstall: arc\n",
      "    Found existing installation: arc 1.0\n",
      "    Uninstalling arc-1.0:\n",
      "      Successfully uninstalled arc-1.0\n",
      "  Running setup.py develop for arc\n",
      "Successfully installed arc-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(make_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fedf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(make_syllables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31921911-61f2-4ffa-b254-be62abb90b37",
   "metadata": {},
   "source": [
    "## Lexicon Generation\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c873dd2-1854-4fa1-a320-a817523e7103",
   "metadata": {},
   "source": [
    "Let's generate 2 lexicons with 4 words each and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:arc.core.lexicon:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=1\n",
      "WARNING:arc.core.lexicon:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 : byːhiːzøː|løːvaːkoː|ʃøːheːpaː|køːsiːmyː\n",
      "1 : buːhoːʃøː|kaːriːfoː|zøːɡɛːmuː|løːvaːkoː\n"
     ]
    }
   ],
   "source": [
    "from arc import make_lexicons\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=2, n_words=4)\n",
    "print(\"\")\n",
    "\n",
    "for i, lexicon in enumerate(lexicons):\n",
    "    print(i, \":\", lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(make_lexicons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a7816d0-f88c-4353-ad5a-e7a137eddd51",
   "metadata": {},
   "source": [
    "> ⚠️ The runtime of this function depends on the parameters when `control_features=True`. If it takes too long, consider reducing the number of words in the lexicon or the number of lexicons. If you don't get any output, consider increasing the maximum pairwise overlap allowed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative feature repetitiveness will be generated first, starting at zero. This means words will be joined into a lexicon if the features of all word pairs in the lexicon have no overlap. If it is not possible to generate the requested number Lexicons with zero overlap, the allowed overlap will be increased untill all lexicons are collected, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eab5ca7e-5929-446b-8a9e-78c979d3b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon: byːhiːzøː|løːvaːkoː|ʃøːheːpaː|køːsiːmyː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n",
      "Lexicon: buːhoːʃøː|kaːriːfoː|zøːɡɛːmuː|løːvaːkoː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"cumulative_feature_repetitiveness:\", lexicon.info[\"cumulative_feature_repetitiveness\"])\n",
    "    print(\"max_pairwise_feature_repetitiveness:\", lexicon.info[\"max_pairwise_feature_repetitiveness\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "## Stream Generation\n",
    "\n",
    "We want to generate a complete set of compatible lexicons for our study, i.e. to generate a compatible set of streams for testing statistical learning hypotheses. If `streams` is empty, try increasing the allowed maximum rythmicity.\n",
    "\n",
    "The function `make_streams` will try to generate one stream for each lexicon and TP mode. If you specify 'max_rhythmicity', it will discard those that do not meet the requirement. By default, all streams from a lexicon will be discarded if the lexicon can't generate streams for all requested TP modes. Printed below you see a collection of streams. Because streams can get long, you only see their key consisting of the lexicon used to generate it and its TP mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69ef37-8b48-4340-b9f8-63a0777dde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_streams\n",
    "help(make_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b0c7f0-281a-4f02-8ac9-3b1621b8a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = make_streams(lexicons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac301ad8-f883-4de7-9719-7db125b37570",
   "metadata": {},
   "source": [
    "> ⚠️ The runtime of this function depends on the parameters, especially when you specify a `max_rhythmicity`, because the function re-samples the random stream until `max_rhythmicity` is satisfied. This takes time, because TP-statistics need to be controlled each time. If it takes too long, consider removing the option.\n",
    "\n",
    "To inspect a stream, select one either by index or by key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "253254b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from arc.types.base_types import Register\n",
    "\n",
    "def write_out_streams(streams: Register, open_in_browser: bool = True, file_name: str = \"\"):\n",
    "\n",
    "    with open('streams.json', 'w') as file:\n",
    "        json.dump({\"streams\": [{\n",
    "            \"stream\": stream.id, \n",
    "            \"info\": {\n",
    "                \"lexicon\": \"|\".join([word.id for word in stream.info[\"lexicon\"]]),\n",
    "                \"rhythmicity_indexes\": stream.info[\"rhythmicity_indexes\"],\n",
    "                \"stream_tp_mode\": stream.info[\"stream_tp_mode\"],\n",
    "                \"n_syllables_per_word\": stream.info[\"n_syllables_per_word\"],\n",
    "                \"n_look_back\": stream.info[\"n_look_back\"],\n",
    "                \"phonotactic_control\": stream.info[\"phonotactic_control\"],\n",
    "                \"syllables_info\": stream.info[\"syllables_info\"],\n",
    "                }} for stream in streams], \"info\": streams.info}, file)\n",
    "\n",
    "    if open_in_browser:\n",
    "        import webbrowser\n",
    "        webbrowser.open('file://' + os.path.realpath(\"streams.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cb365fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out_streams(streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b259e92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'float'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(streams[0].info[\"rhythmicity_indexes\"][\"phon_1_son\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224f751-3cc9-4549-b6e7-b3697f11b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = streams[0]\n",
    "print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e606a1-8427-4d09-9058-f2c0e330b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "print(\"TP mode:\", stream.info[\"stream_tp_mode\"])\n",
    "print(\"Feature PRIs:\") \n",
    "for feat, pri in stream.info[\"rhythmicity_indexes\"].items():\n",
    "    print(\" \", feat, pri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7558308-40fa-4fb5-8ec1-585fc73ba250",
   "metadata": {},
   "source": [
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization/TP-structure mode has been used.\n",
    "\n",
    "This concludes the second tutorial, and we end this series with the third and last tutorial about how to use your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818242aa-4b28-4dcd-a035-85099c2bc1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
