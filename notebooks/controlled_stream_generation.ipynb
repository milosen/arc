{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Tutorial: Controlled Stream Generation\n",
    "\n",
    "We will generate words and a Lexicon with minimal feature overlap between the words. Next, we introduce the 3 main ways to generate random streams, depending on how the transition probabilities (TPs) of their syllables are structured: word-structured TPs, fully random (uniform) TPs, and position-controlled TPs.\n",
    "\n",
    "First, we generate/reload the words register (see arc types tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75e7ba-dc5a-4e87-923a-c9423d498d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from arc import load_words, load_phonemes, make_syllables, make_words\n",
    "\n",
    "FORCE_RECOMPUTE = False\n",
    "\n",
    "if os.path.exists(\"words.json\") and not FORCE_RECOMPUTE:\n",
    "    print(\"Load words...\")\n",
    "    words = load_words(\"words.json\")\n",
    "    print(words)\n",
    "else:\n",
    "    print(\"Load phonemes...\")\n",
    "    phonemes = load_phonemes()\n",
    "    print(phonemes)\n",
    "\n",
    "    print(\"Make syllables...\")\n",
    "    syllables = make_syllables(phonemes)\n",
    "    print(syllables)\n",
    "\n",
    "    print(\"Make words...\")\n",
    "    words = make_words(syllables)\n",
    "    print(words)\n",
    "\n",
    "    print(\"Save words ...\")\n",
    "    words.save(\"words.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413261-d35b-4c93-b6da-ff39cc51805b",
   "metadata": {},
   "source": [
    "## Lexicon\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. Let's start with 4 words each. \n",
    "\n",
    "By default, the function will generate 5 `Lexicon`s max. Let's generate 2 and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_lexicons\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=20, n_words=4)\n",
    "\n",
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"Info:\", lexicon.info)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative overlap between the word features will be generated first, starting at zero overlap. If it is not possible to generate all the requested Lexicons with the given parameters, the allowed overlap will be increased, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "## Compatible Streams\n",
    "\n",
    "We want to generate a complete set of compatible lexicons for our study, i.e. to generate a compatible set of streams for testing statistical learning hypotheses. If `streams` is empty, try increasing the allowed maximum rythmicity).\n",
    "\n",
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization/TP-structure mode has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82199da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_streams\n",
    "streams = make_streams(lexicons, max_rhythmicity=0.1)\n",
    "\n",
    "print(\"Streams Summary:\", streams)\n",
    "print(\"Info:\", streams.info)\n",
    "print(\"\")\n",
    "\n",
    "for stream in streams:\n",
    "    print(\"Stream:\", stream)\n",
    "    stream_info = {k: str(v) for k, v in stream.info.items()}\n",
    "    print(\"Info:\", stream_info)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2386f9f-8816-434d-9846-aa49bab12ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c24ea082-5628-43b0-9478-aa4a25e58705",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f30dbc-4986-487a-8039-f9b7fc99e8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "arc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
