{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Tutorial: Controlled Stream Generation\n",
    "\n",
    "We will generate words and a Lexicon with minimal feature overlap between the words. Then, we will follow up with the 2 main ways we can generate random streams, the `Word`-randomized (TP-structured) and the `Syllable`-randomized (TP-random/uniform) stream generation.\n",
    "\n",
    "First, we generate/reload the words register (see arc types tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75e7ba-dc5a-4e87-923a-c9423d498d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from arc import load_words\n",
    "\n",
    "FORCE_RECOMPUTE = True\n",
    "\n",
    "if os.path.exists(\"words.json\") and not FORCE_RECOMPUTE:\n",
    "    words = load_words(\"words.json\")\n",
    "else:\n",
    "    from arc import load_phonemes\n",
    "    feature_phonemes = load_phonemes()\n",
    "    \n",
    "    from arc.core.syllable import make_feature_syllables\n",
    "    feature_syllables = make_feature_syllables(feature_phonemes, phoneme_pattern=\"cV\")\n",
    "\n",
    "    from arc.io import read_syllables_corpus\n",
    "    german_syllable_corpus = read_syllables_corpus()  # defaults to the german corpus that comes with ARC\n",
    "\n",
    "    syllables_valid_german = feature_syllables.intersection(german_syllable_corpus)\n",
    "\n",
    "    from arc.controls.filter import filter_uniform_syllables, filter_common_phoneme_syllables\n",
    "    print(\"Syllables valid german: \", syllables_valid_german)\n",
    "    \n",
    "    syllables_german_filtered = filter_uniform_syllables(syllables_valid_german)\n",
    "    print(\"Syllables with uniform probability of occurence: \", syllables_german_filtered)\n",
    "    \n",
    "    syllables_german_filtered = filter_common_phoneme_syllables(syllables_german_filtered)\n",
    "    print(\"Syllables with common phonemes: \", syllables_german_filtered)\n",
    "\n",
    "    from arc.core.word import make_words\n",
    "    print(\"Make words...\")\n",
    "    words = make_words(syllables_german_filtered, n_words=10_000, progress_bar=True)\n",
    "\n",
    "    from arc.controls.filter import filter_common_phoneme_words\n",
    "    print(\"Select words with common phonemes (german) ...\")\n",
    "    words = filter_common_phoneme_words(words, position=0)\n",
    "\n",
    "    print(\"Save words ...\")\n",
    "    words.save(\"words.json\")\n",
    "    \n",
    "print(words)\n",
    "\n",
    "# print(\"Select words with common bigrams and trigrams (german) ...\")\n",
    "# from arc.tpc.filter import filter_gram_stats\n",
    "# words = filter_gram_stats(words)\n",
    "\n",
    "print(\"Sample subset of words (n=200) ...\")\n",
    "words = words.get_subset(200)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413261-d35b-4c93-b6da-ff39cc51805b",
   "metadata": {},
   "source": [
    "## Lexicon\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. Let's start with 4 words each. \n",
    "\n",
    "By default, the function will generate 5 `Lexicon`s max. Let's generate 2 and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.controls.lexicon import make_lexicons_from_words\n",
    "\n",
    "lexicons = make_lexicons_from_words(words, n_lexicons=2)\n",
    "\n",
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"Info:\", lexicon.info)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative overlap between the word features will be generated first, starting at zero overlap. If it is not possible to generate all the requested Lexicons with the given parameters, the allowed overlap will be increased, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used.\n",
    "\n",
    "## Stream\n",
    "\n",
    "### Single Stream\n",
    "\n",
    "The stream generation internally generates a Lexicon first, and then a Stream based on that. The following cell, however, generates a stream directly from words for convenience. The cell should execute quickly. If it doesn't, try reducing the number of words per lexicon or increasing the allowed rhythmicity index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd7657-fb12-471b-b7b7-c391d9559f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994a9a6-9991-4089-bb50-cdf8dd8c01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.controls.stream import make_stream_from_words\n",
    "\n",
    "stream = make_stream_from_words(words, tp_mode=\"word_structured\", n_words=4, max_rhythmicity=0.1, max_lexicons=100, max_tries_randomize=10)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(stream)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for key, val in stream.info.items():\n",
    "    print(f\"{key}: {str(val)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917d545-995f-4640-8b2b-53c2884d645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import load_phonemes\n",
    "feature_phonemes = load_phonemes()\n",
    "print(feature_phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization mode has been used. The randomization mode can be `syllable` or `word`. It can be immediately varified that the randomization mode is `word`, since the individual words of the Lexicon can be recognized in the stream. Contrastingly, syllable level randomization means that the words are further brocken down into syllables, and the syllables are shuffled across the whole lexicon, destroying word-level information.\n",
    "\n",
    "Next, we will use this distinction to generate a compatible set of streams for testing statistical learning hypotheses.\n",
    "\n",
    "### Set of Compatible Streams\n",
    "\n",
    "If this runs quickly, then we can step it up and generate a complete set of compatible lexicons for our study. If `streams` is empty, try increasing the allowed maximum rythmicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82199da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.tpc.stream import make_compatible_streams\n",
    "streams = make_compatible_streams(words, n_words=4, max_rhythmicity=0.2)\n",
    "\n",
    "for i, stream in enumerate(streams):\n",
    "    print(\"========= Stream Nr. \", i + 1, \" =========\")\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(stream)\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for key, val in stream.info.items():\n",
    "        print(f\"{key}: {str(val)}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88c818-3a75-4d68-9b05-943f14462acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f30dbc-4986-487a-8039-f9b7fc99e8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "arc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
