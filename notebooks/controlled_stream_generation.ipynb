{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Tutorial: Controlled Stream Generation\n",
    "\n",
    "We will generate words and a Lexicon with minimal feature overlap between the words. Next, we introduce the 3 main ways to generate random streams, depending on how the transition probabilities (TPs) of their syllables are structured: word-structured TPs, fully random (uniform) TPs, and position-controlled TPs.\n",
    "\n",
    "First, we generate/reload the words register (see arc types tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75e7ba-dc5a-4e87-923a-c9423d498d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from arc import load_words, load_phonemes, make_syllables, make_words\n",
    "\n",
    "FORCE_RECOMPUTE = False\n",
    "\n",
    "if os.path.exists(\"words.json\") and not FORCE_RECOMPUTE:\n",
    "    print(\"Load words...\")\n",
    "    words = load_words(\"words.json\")\n",
    "    print(words)\n",
    "else:\n",
    "    print(\"Load phonemes...\")\n",
    "    phonemes = load_phonemes()\n",
    "    print(phonemes)\n",
    "\n",
    "    print(\"Make syllables...\")\n",
    "    syllables = make_syllables(phonemes)\n",
    "    print(syllables)\n",
    "\n",
    "    print(\"Make words...\")\n",
    "    words = make_words(syllables)\n",
    "    print(words)\n",
    "\n",
    "    print(\"Save words ...\")\n",
    "    words.save(\"words.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413261-d35b-4c93-b6da-ff39cc51805b",
   "metadata": {},
   "source": [
    "## Lexicon\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. Let's start with 4 words each. \n",
    "\n",
    "By default, the function will generate 5 `Lexicon`s max. Let's generate 2 and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_lexicons\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=20, n_words=4)\n",
    "\n",
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"Info:\", lexicon.info)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative overlap between the word features will be generated first, starting at zero overlap. If it is not possible to generate all the requested Lexicons with the given parameters, the allowed overlap will be increased, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "## Compatible Streams\n",
    "\n",
    "We want to generate a complete set of compatible lexicons for our study, i.e. to generate a compatible set of streams for testing statistical learning hypotheses. If `streams` is empty, try increasing the allowed maximum rythmicity).\n",
    "\n",
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization/TP-structure mode has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea9483-fdad-470c-9caf-abff48b46a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from arc import Register\n",
    "import numpy as np\n",
    "\n",
    "def print_stream(stream):\n",
    "    stream_dict = {}\n",
    "    stream_dict[\"stream\"] = stream.id\n",
    "    stream_dict[\"stream_info\"] = {}\n",
    "    for k, v in stream.info.items():\n",
    "        if type(v) is Register:\n",
    "            v = str(v)\n",
    "        if type(v) is np.int64:\n",
    "            v = int(v)\n",
    "        if type(v) is tuple:\n",
    "            v = list(v)\n",
    "        stream_dict[\"stream_info\"][k] = v\n",
    "    yaml_str = yaml.dump(stream_dict, allow_unicode=True)\n",
    "    print(yaml_str)\n",
    "\n",
    "def print_streams_summary(streams):\n",
    "    streams_summary = {}\n",
    "    streams_summary[\"streams_info\"] = streams.info\n",
    "    streams_summary[\"streams_info\"][\"lexicons\"] = list(set([str(stream.info[\"lexicon\"]) for stream in streams]))\n",
    "    streams_summary[\"streams_info\"][\"tp_modes\"] = list(streams_summary[\"streams_info\"][\"tp_modes\"])\n",
    "    yaml_str = yaml.dump(streams_summary, allow_unicode=True)\n",
    "    print(yaml_str)\n",
    "    print(\"\")\n",
    "    for stream in streams:\n",
    "        print_stream(stream)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82199da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_streams\n",
    "streams = make_streams(lexicons, max_rhythmicity=0.1)\n",
    "\n",
    "print_streams_summary(streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2debc-1374-4624-ade1-60d9a051995e",
   "metadata": {},
   "source": [
    "# Compare Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c990711-d25b-4a03-b105-c7d400c3be1a",
   "metadata": {},
   "source": [
    "Streams based on controlled lexicons (ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f78b01-78c8-4ba3-8844-c292cbe5b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LEXICONS = 10\n",
    "N_REPS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337f24-4b48-4598-ad47-00714b74921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_streams\n",
    "\n",
    "controlled_streams = make_streams(\n",
    "    lexicons, \n",
    "    max_rhythmicity=None, \n",
    "    num_repetitions=N_REPS,\n",
    "    n_lexicon_streams=N_LEXICONS\n",
    ")\n",
    "\n",
    "print_streams_summary(controlled_streams)\n",
    "\n",
    "len(controlled_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00475fed-bf37-4821-87a7-6962ef72145d",
   "metadata": {},
   "source": [
    "... vs. streams based on random / uncontrolled lexicons (baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16a898-0eeb-4519-ad2e-ad625da4b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_lexicons = make_lexicons(words, n_lexicons=20, n_words=4, control_features=False)\n",
    "\n",
    "random_streams = make_streams(\n",
    "    lexicons, \n",
    "    max_rhythmicity=None, \n",
    "    num_repetitions=N_REPS,\n",
    "    n_lexicon_streams=N_LEXICONS\n",
    ")\n",
    "\n",
    "print_streams_summary(random_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8e397-1ac5-4dac-84bb-106eaae80e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\"control\": [], \"lexicon\": [], \"feature\": [], \"PRI\": [], \"stream_tp_mode\": []}\n",
    "for control, streams in [(\"ours\", controlled_streams), (\"baseline\", random_streams)]:\n",
    "    for stream in streams:\n",
    "        for k, v in stream.info[\"rhythmicity_indexes\"].items():\n",
    "            data[\"feature\"].append(k)\n",
    "            data[\"PRI\"].append(v)\n",
    "            data[\"control\"].append(control)\n",
    "            data[\"lexicon\"].append(str(stream.info[\"lexicon\"]))\n",
    "            data[\"stream_tp_mode\"].append(stream.info[\"stream_tp_mode\"])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86faf394-48b3-4b77-a69c-3595116e2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axs = plt.subplots(3, 1, figsize=(25, 25))\n",
    "\n",
    "for i, tp_mode in enumerate(controlled_streams.info[\"tp_modes\"]):\n",
    "    sns.boxplot(df[df[\"stream_tp_mode\"] == tp_mode], x=\"feature\", y=\"PRI\", hue=\"control\", ax=axs[i])\n",
    "    axs[i].set_title(f\"Stream TP mode: {tp_mode}\")\n",
    "\n",
    "plt.savefig(\"lexicon_pri.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56b23e-5dd5-47be-91ed-b7a345efc35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "arc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
