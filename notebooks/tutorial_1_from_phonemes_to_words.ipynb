{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6dcef0d-76e5-4330-a879-ab8ff03acbd8",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "You will learn basic data structures of the ARC Typesystem, as well as saving and load with the core ARC-Types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c650cf2-9c82-4215-8538-340acc7d1735",
   "metadata": {},
   "source": [
    "## Basic Types\n",
    "\n",
    "There are two types of objects we deal with in ARC:\n",
    "\n",
    "An **Element** is any linguistic object of interest. In our case, `Phoneme`, `Syllable`, `Word`, and `Stream` are Elements. These objects can consist of other elements, in a dictionary-style fashion, i.e. `Word`, and `Stream` consist of `Syllable`s, a `Syllable` consists of `Phoneme`s, and `Phonemes` are atomic. If an Element consists of multiple sub-elements, like in the case of a Syllable, the sub-elements can repeat, e.g. a Phoneme can repeat multiple times inside a Syllable. Elements can be part of a Corpus.  Like in real corpora, every element can be annotated, hence it has an `.info` field, which can hold arbitrary annotations in dictionary format.\n",
    "\n",
    "A **Register** is essentially an ordered set with some extra functionality. We use this container type to create ordered collections of Elements that do not repeat, i.e. ordered sets of Phonemes, Syllables, and Words. Since every element in the Register is unique in its string representation, it can be hashed and thus found quickly in memory. The `Lexicon` type is implemented as a Register of words, but also any corpus of Phonemes or Syllables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4f9a4-e047-435a-b364-b71ad4119ca2",
   "metadata": {},
   "source": [
    "## Phonemes\n",
    "Phonemes are the atomic unit of the ARC-Typesystem and built the basis for constructing other types like Syllables and Words. \n",
    "To enjoy the full functionolity of ARC, you'll need Phonemes annotated with their phonetic features. Luckily, ARC comes with an extensive corpus of Phonemes and phonetic features.\n",
    "Let's load them and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38768ae-6d3a-43cd-943f-394683b7962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import load_phonemes\n",
    "phonemes = load_phonemes()\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11c164-8de3-4604-97cc-7a6f405161c5",
   "metadata": {},
   "source": [
    "The `phonemes` variable is a Collection of Phoneme-Objects, more specifically a `Register`. What you see when you print any Register is a short summary of the first elements.\n",
    "You can treat the Register like most Python collection types, meaning you can access elements, iterate over it etc.\n",
    "\n",
    "> Note: Internally, `Register`s are `OrderedDict`s (with some extra convenience methods). Essentially, you can treat it like both Python builtin types `Dict`and `List`.\n",
    "\n",
    "Let's see that in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We can reference elements of a Corpus by position/index:\", phonemes[0], \", or by its string representation:\", phonemes[\"k\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7d30f",
   "metadata": {},
   "source": [
    "Internally, Elements are `Dict`-like objects, more specifically, [Pydantic](https://docs.pydantic.dev/latest/) types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes[\"k\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7efbca-c661-4510-a0f7-694e5926e12b",
   "metadata": {},
   "source": [
    "Annotations can be referenced via the `.info` property, which can hold arbitrary dictionary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540a558-6740-4873-aa82-943128df52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes[\"k\"].info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26a0de",
   "metadata": {},
   "source": [
    "Phoneme features can be hard to interprete, so you can also get features directly, e.g. the \"is labial\" binary feature, called `lab`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bff672",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes[\"k\"].get_binary_feature(\"lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8b887-ea95-4c32-8289-1e5504754cf9",
   "metadata": {},
   "source": [
    "Finally, you can get some help on which features the binary feature vector holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(phonemes[\"k\"].get_binary_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053135c-8d52-47c0-8557-1edae8041d2a",
   "metadata": {},
   "source": [
    "While Registers in ARC print as compact summaries of there contents, they can be arbitrarily complex data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc4aa7-2c00-4f29-a235-2400a2b4bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the print output:\", phonemes, end=\"\\n\\n\")\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"These are the contents of the Syllable Register:\", end=\"\\n\\n\")\n",
    "for ph in phonemes:\n",
    "    pprint(ph)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a4bfe6-c816-4a9c-9bc7-247db67fa32c",
   "metadata": {},
   "source": [
    "Regardless of the contents, Elements and Registers are always JSON serializable, as long as they are valid (which is checked by Pydantic at initialization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcfb029-2e21-4841-bc84-9aecdb3ecfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc101a9-5470-4222-aa62-11230c5797f6",
   "metadata": {},
   "source": [
    "... which means they can be written to file. The Corpus type has a method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11395597-02eb-4d48-b759-cc83a791cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes.save(\"test_phonemes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed742a-a23f-4522-bb17-c21df7642ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import load_phonemes\n",
    "\n",
    "loaded_phonemes = load_phonemes(\"test_phonemes.json\")\n",
    "print(loaded_phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c94d59-f484-4728-ae83-2b0451266f75",
   "metadata": {},
   "source": [
    "## Syllables\n",
    "Our first composite type is the `Syllable`, consisting of a list of `Phoneme`s. Let's make a collection of syllables, that follow the `cV` pattern, meaning they consist of a single-character phoneme `c` followed by a long vowel `V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a857502-e57a-4aa9-9577-cbf8f3d52ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.generation.syllables import make_feature_syllables\n",
    "artificial_syllables = make_feature_syllables(phonemes, phoneme_pattern=\"cV\")\n",
    "print(artificial_syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(artificial_syllables[\"cʔː\"], artificial_syllables[1])\n",
    "artificial_syllables[\"cʔː\"], artificial_syllables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d441188",
   "metadata": {},
   "source": [
    "Finally, you can iterate over both, the Elements of a Register and over the Sub-Elements of an Element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for syllable in artificial_syllables:\n",
    "    print(\"Syllable\", syllable, f\"consists of phonemes\", end=\"\") \n",
    "    for phoneme in syllable:\n",
    "        print(\" \", end=\"\")\n",
    "        print(phoneme, end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82434a",
   "metadata": {},
   "source": [
    "## Merge and Filter operations for Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138efea-9411-4218-8340-fc1daf393a3d",
   "metadata": {},
   "source": [
    "Since we started with an international Phoneme corpus, and because we generate artificial syllables, there will be many Syllables we do not want to include in our further analysis. Lets filter out some of them.\n",
    "\n",
    "We'll start by filtering based on a real corpus of syllables. ARC comes with an example corpus in German, so let's use that as an example. \n",
    "\n",
    ">The filter-implementations are specific to the german corpus, so you might want to implement your own filters. We will discuss that in a later tutorial. If you are curious, you can take a look at the arc.filter submodule to see how to implement a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35273f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.io import read_syllables_corpus\n",
    "german_syllable_corpus = read_syllables_corpus()  # defaults to the german corpus that comes with ARC\n",
    "print(german_syllable_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afce11c-8228-4ca6-b89f-14ecd0ef564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_syllables_valid_german = artificial_syllables.intersection(german_syllable_corpus)\n",
    "print(artificial_syllables_valid_german)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55ae7e-947e-44ba-8be4-a009ce3c0cca",
   "metadata": {},
   "source": [
    "In our original publication, we filter syllables based on the p-value that the syllable is uniformaly distributed with the others. This can be implemented as a filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f67b45-848d-4228-af5b-00934297b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.filter import filter_uniform_syllables, filter_common_phoneme_syllables\n",
    "\n",
    "syllables_german_filtered = filter_uniform_syllables(artificial_syllables_valid_german)\n",
    "print(\"Syllables with uniform probability of occurence: \", syllables_german_filtered)\n",
    "\n",
    "syllables_german_filtered = filter_common_phoneme_syllables(syllables_german_filtered)\n",
    "print(\"Syllables with common phonemes: \", syllables_german_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3e3c2-a4dd-4db7-9a28-d44b56720ee4",
   "metadata": {},
   "source": [
    "If you have a native (in our case German) phoneme corpus, you can filter the syllables based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf91d4c-f5d4-482c-93e5-dc3d95bde840",
   "metadata": {},
   "source": [
    "## Export to SSML\n",
    "Once we are done making syllables, we can export them to Speech Synthesis Markup Language (SSML) for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837841d-8d86-4e6e-9e86-7a7497480444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.io import export_speech_synthesiser\n",
    "export_speech_synthesiser(syllables_german_filtered, syllables_dir=\"ssml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734630a5-f0f0-46fb-8848-c9324b802cc2",
   "metadata": {},
   "source": [
    "## Words\n",
    "`Word`s are made out of `Syllable`s, same as before when we made syllables from phonemes.\n",
    "\n",
    "Since one of ARC's main features is rythmicity control, our `make_words` function will only create words that have minimum overlap of phonotactic features. By default, this function generates 10000 words, but you can change that with the `n_words` option. With 10000 words, this should run fairly quickly, however, when you set the number higher you may want to also set the `progress_bar=True` flag in the function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32d88d-c32c-47a8-91f9-373c6b0b6020",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from arc.generation.words import make_words\n",
    "words = make_words(syllables_german_filtered, n_words=100_000, progress_bar=True)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3752ac-1da1-441c-977c-ff7d715c44a2",
   "metadata": {},
   "source": [
    "Again, we apply some filters, but this time at the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8bd19-72e3-4a7d-b3d9-e85b49e12795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.filter import filter_common_phoneme_words, filter_gram_stats\n",
    "\n",
    "words_filtered = filter_common_phoneme_words(words, position=0)\n",
    "print(words_filtered)\n",
    "\n",
    "#words_filtered = filter_gram_stats(words_filtered)\n",
    "#print(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546f1ec-89b7-4275-9f3c-22681daa0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_filtered.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896e8bf-1b21-4fd1-b052-8c1d97552694",
   "metadata": {},
   "source": [
    "Even with all the phonotactic conditions we applied, there may be many words left to choose from to build our `Lexicon`s and `Stream`s later on.\n",
    "\n",
    "However, we can always get a random subsample of a Register by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430f124-cb84-42b4-82b7-29e7cd112bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_subset = words_filtered.get_subset(100)\n",
    "print(words_subset, words_subset.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275c803-2730-4976-ac80-ecaa80e6b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_subset.save(\"words.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fbe3e-b21d-437e-b1c3-b2588b75e7c0",
   "metadata": {},
   "source": [
    "This concludes our first tutorial. \n",
    "We've made `Syllable`s from `Phonemes`s and `Word`s from `Syllable`s and applied filters to them. \n",
    "Finally, we saved the generated words to a json file. \n",
    "In the next tutorial, we will pick up where we left and load the saved words to generate a `Lexicon`, a Register of `Word`s with specific phonotactic requirements. Later, we will use Lexicons to generate different types of streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28ff53-a309-428c-997a-c69955502184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "arc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
