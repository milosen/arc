{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6dcef0d-76e5-4330-a879-ab8ff03acbd8",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "You will learn basic data saving and load with the core ARC-Types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4f9a4-e047-435a-b364-b71ad4119ca2",
   "metadata": {},
   "source": [
    "## Phonemes\n",
    "Phonemes are the atomic unit of the ARC-Typesystem and built the basis for constructing other types like Syllables and Words. \n",
    "To enjoy the full functionolity of ARC, you'll need Phonemes with the phonetic feature fields filled. Luckily, ARC comes with an extensive corpus of Phonemes and phonetic features.\n",
    "Let's load them and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38768ae-6d3a-43cd-943f-394683b7962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k͡p|ɡ͡b|c|ɡ|k|q|ɖ|ɟ|ɠ|ɢ|... (5175 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc import load_default_phonemes\n",
    "phonemes = load_default_phonemes()\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11c164-8de3-4604-97cc-7a6f405161c5",
   "metadata": {},
   "source": [
    "The `phonemes` variable is a Collection of Phoneme-Objects, more specifically an `ARC-Collection`. What you see when you print any `ARC-Collection` is a short summary of the highest level elements.\n",
    "You can treat the `ARC-Collection` like most Python collection types, meaning you can access elements, iterate over it etc.\n",
    "\n",
    "> Note: Internally, `ARC-Collection`s are `OrderedDict`s (with some extra convenience methods). This meaning you can treat it like both Python builtin types `Dict`and `List`.\n",
    "\n",
    "Let's see that in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c94d59-f484-4728-ae83-2b0451266f75",
   "metadata": {},
   "source": [
    "## Syllables\n",
    "Our first composite type is the `Syllable`, consisting of a list of `Phoneme`s. Let's make a collection of syllables, that follow the `cV`pattern, meaning they consist of a single-character phoneme `c` followed by a long vowel `V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a857502-e57a-4aa9-9577-cbf8f3d52ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64564f490ea84500ae1d1eed3851b1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cʔː|cɥː|cɰː|cʋː|cʍː|cjː|cwː|cɹː|cɻː|cɑː|... (2108 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc.data import make_feature_syllables\n",
    "syllables = make_feature_syllables(phonemes, phoneme_pattern=\"cV\")\n",
    "print(syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138efea-9411-4218-8340-fc1daf393a3d",
   "metadata": {},
   "source": [
    "Since we started with an international Phoneme corpus, there may be many Syllables, that we do not want to include in our further analysis. Lets filter out some of them.\n",
    "\n",
    "We'll start by filtering based on a corpus of syllables. ARC comes with an example corpus in German, and it will be called, when you call filters without supplying a path to a custom file. \n",
    "\n",
    ">The filter-implementations are specific to the corpus, so you might want to implement your own filters. We will discuss that in a later tutorial. If you are curious, you can take a look at the arc.filter submodule to see how to implement a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afce11c-8228-4ca6-b89f-14ecd0ef564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daː|diː|viː|ziː|zoː|taː|keː|haː|ɡeː|neː|... (130 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc.filter import filter_with_corpus\n",
    "syllables = filter_with_corpus(syllables)\n",
    "print(syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55ae7e-947e-44ba-8be4-a009ce3c0cca",
   "metadata": {},
   "source": [
    "In our original publication, we filter syllables based on the p-value that the syllable is uniformaly distributed with the others. We made a filter for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f67b45-848d-4228-af5b-00934297b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zuː|toː|moː|deː|biː|niː|kaː|buː|beː|ruː|... (77 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc.filter import filter_uniform_syllables\n",
    "syllables = filter_uniform_syllables(syllables)\n",
    "print(syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3e3c2-a4dd-4db7-9a28-d44b56720ee4",
   "metadata": {},
   "source": [
    "If you have a native (in our case German) phoneme corpus as well, you can filter the syllables based on that.\n",
    "\n",
    "> Note that we could have done that at the phoneme level already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3de342-d19a-4eaa-9216-f00648eeb61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeff73d34fe1466e8dd468e5ab6b44a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zuː|toː|moː|deː|biː|niː|kaː|buː|beː|ruː|... (76 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc.filter import filter_common_phoneme_syllables\n",
    "syllables = filter_common_phoneme_syllables(syllables)\n",
    "print(syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf91d4c-f5d4-482c-93e5-dc3d95bde840",
   "metadata": {},
   "source": [
    "## Export to SSML\n",
    "Once we are done choosing syllables, we can export them to Speech Synthesis Markup Language (SSML) for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7837841d-8d86-4e6e-9e86-7a7497480444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc.io import export_speech_synthesiser\n",
    "export_speech_synthesiser(syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734630a5-f0f0-46fb-8848-c9324b802cc2",
   "metadata": {},
   "source": [
    "## Words\n",
    "`Word`s are made out of `Syllable`s, same as before when we made syllables from phonemes.\n",
    "\n",
    "Since one of ARC's main features is rythmicity control, our `make_words` function will only create words that have minimum overlap of phonotactic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e32d88d-c32c-47a8-91f9-373c6b0b6020",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad095e9864d34982b3d087fafab46aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siːbøːheː|laːɡyːfuː|myːtiːçaː|toːheːfaː|boːlyːçaː|ʃuːmɛːɡyː|ʃɛːkaːmyː|ɡɛːzyːmuː|byːʃaːheː|niːpoːçaː|... (5501 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc.data import make_words\n",
    "words = make_words(syllables)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3752ac-1da1-441c-977c-ff7d715c44a2",
   "metadata": {},
   "source": [
    "Again, we apply some filters, but this time at word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a8bd19-72e3-4a7d-b3d9-e85b49e12795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff772b9093a94996940277149f16701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laːɡyːfuː|myːtiːçaː|toːheːfaː|boːlyːçaː|ʃuːmɛːɡyː|ʃɛːkaːmyː|ɡɛːzyːmuː|byːʃaːheː|niːpoːçaː|myːkɛːʃaː|... (5089 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc.filter import filter_common_onset_words\n",
    "words = filter_common_onset_words(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5546f1ec-89b7-4275-9f3c-22681daa0fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Dropping trigram 'bɪs' with conflicting stats {'freq': 1548, 'p_unif': 0.0} != {'freq': 2645, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'ɐʊn' with conflicting stats {'freq': 430, 'p_unif': 0.0} != {'freq': 466, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'əʊn' with conflicting stats {'freq': 319, 'p_unif': 0.0} != {'freq': 325, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'ɐneː' with conflicting stats {'freq': 289, 'p_unif': 0.0} != {'freq': 299, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'seːɐ' with conflicting stats {'freq': 225, 'p_unif': 0.0} != {'freq': 228, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'oːvɪ' with conflicting stats {'freq': 180, 'p_unif': 0.0} != {'freq': 184, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'tjeː' with conflicting stats {'freq': 136, 'p_unif': 0.0} != {'freq': 137, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'auçf' with conflicting stats {'freq': 134, 'p_unif': 0.0} != {'freq': 137, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'daːj' with conflicting stats {'freq': 123, 'p_unif': 0.0} != {'freq': 125, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'çkoː' with conflicting stats {'freq': 123, 'p_unif': 0.0} != {'freq': 125, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'noːp' with conflicting stats {'freq': 123, 'p_unif': 0.0} != {'freq': 124, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'ɛŋt' with conflicting stats {'freq': 112, 'p_unif': 0.0} != {'freq': 115, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'laːf' with conflicting stats {'freq': 112, 'p_unif': 0.0} != {'freq': 115, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'fɛŋ' with conflicting stats {'freq': 100, 'p_unif': 0.0} != {'freq': 103, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'çfa' with conflicting stats {'freq': 100, 'p_unif': 0.0} != {'freq': 103, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'neːz' with conflicting stats {'freq': 98, 'p_unif': 0.0} != {'freq': 100, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'auçə' with conflicting stats {'freq': 97, 'p_unif': 0.0} != {'freq': 98, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'uːtv' with conflicting stats {'freq': 87, 'p_unif': 0.0} != {'freq': 88, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'aːviː' with conflicting stats {'freq': 80, 'p_unif': 0.0} != {'freq': 81, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'oːkd' with conflicting stats {'freq': 77, 'p_unif': 0.0} != {'freq': 78, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'hiːs' with conflicting stats {'freq': 62, 'p_unif': 0.0} != {'freq': 63, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'ɪmm' with conflicting stats {'freq': 58, 'p_unif': 0.0} != {'freq': 1777, 'p_unif': 0.0}.\n",
      "WARNING:root:Dropping trigram 'koːn' with conflicting stats {'freq': 44, 'p_unif': 0.12599231109694486} != {'freq': 45, 'p_unif': 0.11354545074607303}.\n",
      "WARNING:root:Dropping trigram 'xfiː' with conflicting stats {'freq': 40, 'p_unif': 0.17878099562598526} != {'freq': 41, 'p_unif': 0.165104696421566}.\n",
      "WARNING:root:Dropping trigram 'ɛːdɛ' with conflicting stats {'freq': 38, 'p_unif': 0.20719040101624486} != {'freq': 39, 'p_unif': 0.19280356678672972}.\n",
      "WARNING:root:Dropping trigram 'tleː' with conflicting stats {'freq': 36, 'p_unif': 0.23713617903133255} != {'freq': 37, 'p_unif': 0.2219609290443415}.\n",
      "WARNING:root:Dropping trigram 'xəd' with conflicting stats {'freq': 35, 'p_unif': 0.2527389566166275} != {'freq': 36, 'p_unif': 0.23713617903133255}.\n",
      "WARNING:root:Dropping trigram 'aifə' with conflicting stats {'freq': 28, 'p_unif': 0.376329684901887} != {'freq': 29, 'p_unif': 0.35689393716719686}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toːheːfaː|deːfaːhiː|loːfiːkuː|fuːnyːɡiː|piːzuːhoː|luːɡaːfoː|hiːbaːsuː|luːkaːfyː|hoːtiːvaː|biːseːhoː|... (1130 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc.filter import filter_gram_stats\n",
    "words = filter_gram_stats(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896e8bf-1b21-4fd1-b052-8c1d97552694",
   "metadata": {},
   "source": [
    "Even with all the phonotactic conditions we applied, there are still many words to choose from to build our `Lexicons`and streams later on.\n",
    "\n",
    "You can always get a random subsample of a collection by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a430f124-cb84-42b4-82b7-29e7cd112bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ʃeːhoːbøː|riːfoːkuː|huːfiːdoː|reːfoːɡiː|fuːniːkaː|ɡaːsuːmyː|laːkuːfoː|zuːmoːɡɛː|faːryːkoː|faːhoːtyː|... (100 elements total)\n"
     ]
    }
   ],
   "source": [
    "words = words.sample(100)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8275c803-2730-4976-ac80-ecaa80e6b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fbe3e-b21d-437e-b1c3-b2588b75e7c0",
   "metadata": {},
   "source": [
    "This concludes our first tutorial. You've made `Syllable`s from `Phonemes`s and `Word`s from `Syllable`s and applyied filters to them. Finally, you saved the generated words to a json file. In the next tutorial, we will pick up where we left and load these words to generate a `Lexcion`, a list of `Word`s with specific phonotactic requirements and use the lexicon to generate different types of streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28ff53-a309-428c-997a-c69955502184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "arc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
