{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e2debc-1374-4624-ade1-60d9a051995e",
   "metadata": {},
   "source": [
    "# Compare Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39428d",
   "metadata": {},
   "source": [
    "We compare the streams generated with \n",
    "\n",
    "1. controlled lexicons (ours),\n",
    "2. random baseline streams, and\n",
    "3. streams generated based on reference lexicons from the literature\n",
    "\n",
    "based on the repetitiveness of the phoneme features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75e7ba-dc5a-4e87-923a-c9423d498d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import load_phonemes, make_syllables, make_words\n",
    "from arc import make_lexicons, load_words\n",
    "\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "words = load_words(\"words.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1cfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import pandas as pd\n",
    "from arc.types.base_types import Register\n",
    "import numpy as np\n",
    "\n",
    "from arc.core.syllable import LABELS_C, LABELS_V, syllable_from_phonemes\n",
    "from arc.core.word import Word, word_overlap_matrix\n",
    "\n",
    "phonemes = load_phonemes()\n",
    "\n",
    "syll_feature_labels = [LABELS_C, LABELS_V]\n",
    "syllable_type = \"cv\"\n",
    "\n",
    "def to_phoneme(phoneme):\n",
    "    return phoneme\n",
    "\n",
    "def to_syllable(syllable):\n",
    "    if len(syllable) == 3 and not syllable.endswith(\"ː\"):\n",
    "        syllable_obj = syllable_from_phonemes(phonemes, syllable[:2], syll_feature_labels)\n",
    "        syllable_obj.id = syllable\n",
    "        return syllable_obj\n",
    "    return syllable_from_phonemes(phonemes, syllable, syll_feature_labels)\n",
    "\n",
    "def to_word(word):\n",
    "    syllables_list = list(map(to_syllable, word))\n",
    "    word_id = \"\".join(s.id for s in syllables_list)\n",
    "    word_features = list(list(tup) for tup in zip(*[s.info[\"binary_features\"] for s in syllables_list]))\n",
    "    return Word(id=word_id, info={\"binary_features\": word_features}, syllables=syllables_list)\n",
    "\n",
    "def to_lexicon(lexicon):\n",
    "    word_objs_list = list(map(to_word, lexicon))\n",
    "    lexicon = Register({w.id:  w for w in word_objs_list})\n",
    "    lexicon.info.update({\"syllable_feature_labels\": [LABELS_C, LABELS_V],  \"syllable_type\": syllable_type})\n",
    "    overlap = word_overlap_matrix(lexicon)\n",
    "    lexicon.info[\"cumulative_feature_repetitiveness\"] = np.triu(overlap, 1).sum()\n",
    "    lexicon.info[\"max_pairwise_feature_repetitiveness\"] = np.triu(overlap, 1).max()\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f78b01-78c8-4ba3-8844-c292cbe5b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LEXICONS = 21  # number of lexicons per TP mode\n",
    "N_WORDS_PER_LEXICON = 4  \n",
    "N_REPS = 5  # how often to randomize the lexicon to build the total stream, \n",
    "            # i.e. how long will the streams be in lexicon lengths N_REPS*N_WORDS_PER_LEXICON = n words in the stream\n",
    "N_STREAMS_PER_INPUT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742895df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream_info(stream):\n",
    "    print(\"Stream:\", \"|\".join([syll.id for syll in stream]))\n",
    "    print(\"TP mode:\", stream.info[\"stream_tp_mode\"])\n",
    "    print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "    print(\"Feature PRIs:\", stream.info[\"rhythmicity_indexes\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764ef8b",
   "metadata": {},
   "source": [
    "### ARC Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337f24-4b48-4598-ad47-00714b74921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_streams\n",
    "from arc import make_lexicons, Register, load_phonemes, load_words\n",
    "\n",
    "print(\"Load words...\")\n",
    "words = load_words(\"words.json\")\n",
    "\n",
    "print(\"Make lexicons...\")\n",
    "controlled_lexicons = make_lexicons(words, n_lexicons=N_LEXICONS, n_words=N_WORDS_PER_LEXICON, control_features=True)\n",
    "\n",
    "for i, lex in enumerate(controlled_lexicons):\n",
    "    print(i, lex)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example Lexicon:\", controlled_lexicons[0])\n",
    "print(\"Info:\", controlled_lexicons[0].info)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87529389",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlled_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        controlled_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        controlled_streams.append(stream)\n",
    "\n",
    "print_stream_info(controlled_streams[0])\n",
    "\n",
    "len(controlled_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00475fed-bf37-4821-87a7-6962ef72145d",
   "metadata": {},
   "source": [
    "### Random / uncontrolled lexicons (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_lexicons = make_lexicons(words, n_lexicons=N_LEXICONS, n_words=N_WORDS_PER_LEXICON, control_features=False)\n",
    "\n",
    "for i, lex in enumerate(random_lexicons):\n",
    "    print(i, lex)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example Lexicon:\", random_lexicons[0])\n",
    "print(\"Info:\", random_lexicons[0].info)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16a898-0eeb-4519-ad2e-ad625da4b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        random_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        random_streams.append(stream)\n",
    "        \n",
    "print_stream_info(random_streams[0])\n",
    "\n",
    "len(random_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc5056",
   "metadata": {},
   "source": [
    "### Reference lexicons from the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2490522",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicons = [\n",
    " [['pi', 'ɾu', 'ta'],\n",
    "  ['ba', 'ɡo', 'li'],\n",
    "  ['to', 'ku', 'da'],\n",
    "  ['ɡu', 'ki', 'bo']],\n",
    " [['pa', 'be', 'la'],\n",
    "  ['di', 'ne', 'ka'],\n",
    "  ['lu', 'fa', 'ri'],\n",
    "  ['xi', 'so', 'du']],\n",
    " [['ma', 'xu', 'pe'],\n",
    "  ['xe', 'ro', 'ɡa'],\n",
    "  ['de', 'mu', 'si'],\n",
    "  ['fo', 'le', 'ti']],\n",
    " [['pu', 'ke', 'mi'],\n",
    "  ['ra', 'fi', 'nu'],\n",
    "  ['bi', 'na', 'po'],\n",
    "  ['me', 'do', 'xi']],\n",
    " [['no', 'ni', 'xe'],\n",
    "  ['bu', 'lo', 'te'],\n",
    "  ['re', 'mo', 'fu'],\n",
    "  ['ko', 'tu', 'sa']],\n",
    " [['mi', 'lo', 'de'],\n",
    "  ['da', 'le', 'bu'],\n",
    "  ['no', 'ru', 'pa'],\n",
    "  ['ka', 'te', 'xi']],\n",
    " [['ne', 'do', 'li'],\n",
    "  ['ri', 'fo', 'nu'],\n",
    "  ['ba', 'to', 'ɡu'],\n",
    "  ['ki', 'ra', 'pu']],\n",
    " [['ɡo', 'na', 'be'],\n",
    "  ['mu', 'di', 'la'],\n",
    "  ['ro', 'ni', 'xe'],\n",
    "  ['pi', 'ku', 'sa']],\n",
    " [['fu', 'bi', 're'],\n",
    "  ['xe', 'tu', 'si'],\n",
    "  ['ta', 'fi', 'ko'],\n",
    "  ['ke', 'ma', 'po']],\n",
    " [['ti', 'fa', 'xu'],\n",
    "  ['so', 'du', 'xi'],\n",
    "  ['me', 'lu', 'bo'],\n",
    "  ['ɡa', 'ni', 'pe']],\n",
    " [['mi', 'po', 'la'],\n",
    "  ['za', 'bɛ', 'tu'],\n",
    "  ['ʁo', 'ki', 'sɛ'],\n",
    "  ['nu', 'ɡa', 'di']],\n",
    " [['dɛ', 'mʊ', 'ri'],\n",
    "  ['sɛ', 'ni', 'ɡɛ'],\n",
    "  ['ræ', 'ku', 'səʊ'],\n",
    "  ['pi', 'lɛ', 'ru']],\n",
    " [['ki', 'fəʊ', 'bu'],\n",
    "  ['lu', 'fɑ', 'ɡi'],\n",
    "  ['pæ', 'beɪ', 'lɑ'],\n",
    "  ['tɑ', 'ɡəʊ', 'fʊ']],\n",
    " [['bi', 'du', 'pɛ'],\n",
    "  ['məʊ', 'bɑ', 'li'],\n",
    "  ['rɛ', 'ɡæ', 'tʊ'],\n",
    "  ['sæ', 'tɛ', 'kəʊ']],\n",
    " [['bəʊ', 'dɑ', 'mɛ'],\n",
    "  ['fi', 'nəʊ', 'pɑ'],\n",
    "  ['ɡʊ', 'rɑ', 'təʊ'],\n",
    "  ['ləʊ', 'kæ', 'neɪ']],\n",
    " [['fɛ', 'si', 'nɑ'],\n",
    "  ['kɛ', 'su', 'dəʊ'],\n",
    "  ['mæ', 'pʊ', 'di'],\n",
    "  ['ti', 'mi', 'nu']],\n",
    " [['tu', 'pi', 'ɹoʊ'],\n",
    "  ['ɡoʊ', 'la', 'bu'],\n",
    "  ['pa', 'doʊ', 'ti'],\n",
    "  ['bi', 'da', 'ku']],\n",
    " [['meɪ', 'lu', 'ɡi'],\n",
    "  ['ɹa', 'fi', 'nu'],\n",
    "  ['pu', 'keɪ', 'mi'],\n",
    "  ['toʊ', 'na', 'poʊ']],\n",
    " [['ɡoʊ', 'la', 'tu'],\n",
    "  ['da', 'ɹoʊ', 'pi'],\n",
    "  ['ti', 'bu', 'doʊ'],\n",
    "  ['pa', 'bi', 'ku']],\n",
    " [['poʊ', 'fi', 'mu'],\n",
    "  ['noʊ', 'vu', 'ka'],\n",
    "  ['vi', 'koʊ', 'ɡa'],\n",
    "  ['ba', 'fu', 'ɡi']],\n",
    " [['ma', 'nu', 'toʊ'],\n",
    "  ['ni', 'moʊ', 'lu'],\n",
    "  ['voʊ', 'ɹi', 'fa'],\n",
    "  ['li', 'du', 'ɹa']]]\n",
    "\n",
    "ref_lexicons = list(map(to_lexicon, lexicons))\n",
    "\n",
    "for i, lex in enumerate(ref_lexicons):\n",
    "    print(i, lex)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example Lexicon:\", ref_lexicons[0])\n",
    "print(\"Info:\", ref_lexicons[0].info)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        ref_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        ref_streams.append(stream)\n",
    "\n",
    "print_stream_info(ref_streams[0])\n",
    "\n",
    "len(ref_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631123d",
   "metadata": {},
   "source": [
    "### Collect Results\n",
    "\n",
    "We collect all stream generation results and their feature repetitiveness scores in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8e397-1ac5-4dac-84bb-106eaae80e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\"Control\": [], \"Lexicon\": [], \"Feature\": [], \"PRI\": [], \"Stream TP mode\": [], \"Stream\": []}\n",
    "\n",
    "mode_to_mode = {  # TP-random position-random; TP-random position-fixed and TP-structured\n",
    "    \"random\": \"TP-random position-random\",\n",
    "    \"word_structured\": \"TP-structured\",\n",
    "    \"position_controlled\": \"TP-random position-fixed\"\n",
    "}\n",
    "\n",
    "for control, streams in [(\"Controlled lexicons (ARC)\", controlled_streams), (\"Reference lexicons (Literature)\", ref_streams), (\"Random lexicons (Baseline)\", random_streams)]:\n",
    "    for stream in streams:\n",
    "        for k, v in stream.info[\"rhythmicity_indexes\"].items():\n",
    "            data[\"Feature\"].append(k)\n",
    "            data[\"PRI\"].append(v)\n",
    "            data[\"Control\"].append(control)\n",
    "            data[\"Lexicon\"].append(str(stream.info[\"lexicon\"]))\n",
    "            data[\"Stream TP mode\"].append(mode_to_mode[stream.info[\"stream_tp_mode\"]])\n",
    "            data[\"Stream\"].append(\"|\".join(syll.id for syll in stream))\n",
    "        data[\"Feature\"].append(\"max\")\n",
    "        data[\"PRI\"].append(max(stream.info[\"rhythmicity_indexes\"].values()))\n",
    "        data[\"Control\"].append(control)\n",
    "        data[\"Lexicon\"].append(str(stream.info[\"lexicon\"]))\n",
    "        data[\"Stream TP mode\"].append(mode_to_mode[stream.info[\"stream_tp_mode\"]])\n",
    "        data[\"Stream\"].append(\"|\".join(syll.id for syll in stream))\n",
    "\n",
    "df = pd.DataFrame(data).sort_values([\"Control\", \"Lexicon\", \"Stream TP mode\"]).reset_index(drop=True)\n",
    "\n",
    "import os\n",
    "os.makedirs(\"results/\", exist_ok=True)\n",
    "df.to_csv(\"results/full_dataset.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is always randomness in the generation of the lexicons etc., so if you want the exact data from the publication uncomment below \n",
    "# df = pd.read_csv(\"full_dataset.csv\")\n",
    "\n",
    "df_lexicons = df[[\"Control\", \"Lexicon\"]].drop_duplicates().reset_index(drop=True)\n",
    "df_lexicons.to_csv(\"results/all_lexicons.csv\")\n",
    "df_lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2653d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "from pingouin import ttest\n",
    "\n",
    "# There is always randomness in the generation of the lexicons etc., so if you want the exact data from the publication uncomment below \n",
    "# df = pd.read_csv(\"full_dataset.csv\")\n",
    "\n",
    "tp_modes_pretty = [\"TP-random position-random\", \"TP-random position-fixed\", \"TP-structured\"]\n",
    "dfs = []\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Controlled lexicons (ARC)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Reference lexicons (Literature)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"controlled vs. reference {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Controlled lexicons (ARC)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Random lexicons (Baseline)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"controlled vs. random baseline {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Reference lexicons (Literature)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Random lexicons (Baseline)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"reference vs. random baseline {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "ttest_df = pd.concat(dfs)\n",
    "\n",
    "display(ttest_df)\n",
    "\n",
    "ttest_df.to_csv(\"results/ttest_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbff8f",
   "metadata": {},
   "source": [
    "## Example ARC Lexicon From Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_arc_lexicon = to_lexicon([[\"heː\", \"doː\", \"faː\"], [\"riː\", \"foː\", \"ɡyː\"], [\"ʃuː\", \"hiː\", \"boː\"], [\"vaː\", \"kuː\", \"niː\"]])\n",
    "print(\"Example Lexicon:\", example_arc_lexicon)\n",
    "\n",
    "streams = make_streams(\n",
    "        [example_arc_lexicon], \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        )\n",
    "\n",
    "for stream in streams:\n",
    "        print_stream_info(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d25f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
