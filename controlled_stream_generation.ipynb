{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Controlled Stream Generation\n",
    "\n",
    "We will generate words and a lexicon with minimal feature overlap between words. Next, we introduce the 3 main ways to generate random streams based on a lexicon. Each specifies how the transition probabilities (TPs) of their syllables are structured:\n",
    "\n",
    "1. uniformlly distributed TPs, called \"TP-random position-random\" in the paper, \n",
    "2. position-controlled TPs, called \"TP-random position-fixed\", and\n",
    "3. TPs that fully preserve the words, called \"TP-structured\".\n",
    "\n",
    "## Syllable and Word Generation\n",
    "\n",
    "First, we generate/reload the words register (see arc types tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c75e7ba-dc5a-4e87-923a-c9423d498d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load phonemes...\n",
      "k͡p|ɡ͡b|c|ɡ|k|q|ɖ|ɟ|ɠ|ɢ|... (5275 elements total)\n",
      "Make syllables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [23:56:58<00:00,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɡaː|ɡiː|ɡyː|ɡɛː|kaː|koː|kuː|køː|kɛː|baː|... (76 elements total)\n",
      "Make words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9988/10000 [00:11<00:00, 210.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram control...\n",
      "trigram control...\n",
      "positional control...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 457.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loːkuːfiː|nyːfaːkuː|huːfiːtyː|moːzuːɡaː|tɛːheːfoː|raːkɛːfiː|nuːkoːfaː|kaːfuːnɛː|hiːtoːfyː|kuːnɛːfoː|... (1839 elements total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from arc import load_phonemes, make_syllables, make_words\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "\n",
    "print(\"Load phonemes...\")\n",
    "phonemes = load_phonemes()\n",
    "print(phonemes)\n",
    "\n",
    "print(\"Make syllables...\")\n",
    "syllables = make_syllables(phonemes, phoneme_pattern=\"cV\", unigram_control=True, language_alpha=0.05)\n",
    "print(syllables)\n",
    "\n",
    "print(\"Make words...\")\n",
    "words = make_words(syllables, n_words=10_000, max_tries=100_000)\n",
    "print(words)\n",
    "\n",
    "#print(\"Save words ...\")\n",
    "#words.save(\"words.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413261-d35b-4c93-b6da-ff39cc51805b",
   "metadata": {},
   "source": [
    "## Lexicon Generation\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. Let's start with 4 words each. \n",
    "\n",
    "By default, the function will generate 5 `Lexicon`s max. Let's generate 2 and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=1\n",
      "WARNING:root:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon: nuːkaːfoː|faːhoːtiː|kuːriːfyː|zyːbeːhuː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n",
      "Lexicon: hoːdeːfiː|peːhuːʃoː|ɡiːfaːnuː|tiːheːvaː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from arc import make_lexicons, load_words\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=2, n_words=4, control_features=True)\n",
    "print(\"\")\n",
    "\n",
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"cumulative_feature_repetitiveness:\", lexicon.info[\"cumulative_feature_repetitiveness\"])\n",
    "    print(\"max_pairwise_feature_repetitiveness:\", lexicon.info[\"max_pairwise_feature_repetitiveness\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative feature repetitiveness will be generated first, starting at zero. This means words will be joined into a lexicon if the features of all word pairs in the lexicon have no overlap. If it is not possible to generate the requested number Lexicons with zero overlap, the allowed overlap will be incremented untill all lexicons are collected, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "## Stream Generation\n",
    "\n",
    "We want to generate a complete set of compatible lexicons for our study, i.e. to generate a compatible set of streams for testing statistical learning hypotheses. If `streams` is empty, try increasing the allowed maximum rythmicity.\n",
    "\n",
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization/TP-structure mode has been used.\n",
    "\n",
    "The function `make_streams` will try to generate one stream for each lexicon and TP mode, and will discard those that do not meet the max_rhythmicity requirement. By default, all streams from a lexicon will be discarded if the lexicon can't generate streams for all requested TP modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82199da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=1\n",
      "WARNING:root:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream: høː|muː|koː|poː|ryː|foː|deː|ʃiː|fuː|ʃoː|ɡiː|huː|ʃiː|høː|poː|foː|koː|deː|ryː|ʃoː|muː|huː|fuː|ɡiː|muː|deː|høː|fuː|poː|ʃiː|ɡiː|foː|ʃoː|koː|huː|ryː|koː|ɡiː|ʃiː|poː|huː|foː|fuː|ryː|deː|muː|ʃoː|høː|ʃoː|deː|koː|ʃiː|foː|ɡiː|høː|ryː|muː|poː|fuː|huː|deː|poː|ʃoː|ryː|ʃiː|muː|ɡiː|koː|fuː|foː|høː|huː|muː|foː|ryː|huː|ɡiː|poː|koː|høː|ʃiː|ʃoː|fuː|deː|fuː|muː|høː|deː|huː|poː|ɡiː|ʃoː|foː|ʃiː|koː|ryː|høː|foː|muː|ryː|poː|deː|ɡiː|fuː|koː|ʃoː|ʃiː|huː|høː|ɡiː|ryː|fuː|ʃiː|deː|foː|poː|muː|huː|ʃoː|koː|muː|fuː|høː|koː|foː|huː|deː|ʃoː|poː|ʃiː|ryː|ɡiː|deː|muː|ʃiː|poː|høː|fuː|foː|huː|koː|ryː|ɡiː|ʃoː|huː|muː|fuː|høː|deː|ɡiː|ryː|koː|ʃoː|poː|foː|ʃiː|huː|ʃiː|foː|ɡiː|deː|høː|koː|poː|ʃoː|muː|ryː|fuː|ʃiː|høː|foː|ʃoː|huː|poː|fuː|deː|ryː|muː|koː|ɡiː|foː|muː|ʃoː|fuː|ɡiː|poː|ryː|huː|høː|ʃiː|deː|koː|høː|ryː|foː|poː|muː|deː|ʃoː|ʃiː|fuː|koː|huː|ɡiː|huː|fuː|ryː|deː|poː|koː|foː|høː|muː|ɡiː|ʃiː|ʃoː|foː|koː|ʃiː|muː|høː|huː|ryː|ʃoː|deː|fuː|poː|ɡiː|koː|muː|ʃiː|ɡiː|fuː|huː|ʃoː|ryː|høː|poː|deː|foː|deː|ʃiː|ryː|poː|huː|koː|fuː|ʃoː|høː|ɡiː|muː|foː|ryː|ʃiː|koː|deː|huː|foː|fuː|muː|poː|høː|ʃoː|ɡiː|høː|koː|fuː|ʃiː|foː|muː|huː|ɡiː|ʃoː|ryː|poː|deː|ʃiː|deː|muː|ʃoː|ɡiː|poː|foː|koː|ryː|huː|fuː|høː|muː|foː|fuː|koː|høː|ʃiː|ʃoː|poː|huː|deː|ryː|ɡiː|høː|deː|huː|ʃiː|ɡiː|ryː|koː|foː|ʃoː|muː|fuː|poː|ɡiː|koː|muː|poː|høː|fuː|huː|ʃoː|deː|foː|ryː|ʃiː|ryː|fuː|muː|høː|poː|ʃoː|koː|huː|foː|deː|ɡiː|ʃiː|muː|ryː|deː|fuː|foː|ɡiː|huː|koː|poː|ʃiː|høː|ʃoː|foː|ʃiː|huː|poː|muː|koː|ʃoː|fuː|ryː|høː|ɡiː|deː|koː|deː|høː|huː|ryː|muː|ɡiː|foː|poː|fuː|ʃoː|ʃiː|poː|koː|ɡiː|muː|ʃiː|fuː|deː|ʃoː|huː|høː|ryː|foː|huː|muː|deː|poː|ryː|ʃoː|høː|foː|koː|ʃiː|ɡiː|fuː|ɡiː|deː|ryː|foː|høː|muː|ʃoː|poː|fuː|koː|huː|ʃiː|koː|poː|høː|ryː|huː|ɡiː|ʃoː|muː|foː|ʃiː|fuː|deː|muː|deː|huː|fuː|ʃiː|koː|ɡiː|høː|poː|foː|ʃoː|ryː|deː|koː|høː|ʃoː|huː|foː|ryː|ʃiː|poː|ɡiː|fuː|muː|poː|koː|fuː|ryː|ʃoː|høː|ʃiː|muː|huː|deː|foː|ɡiː|foː|poː|ryː|fuː|ɡiː|koː|muː|høː|deː|ʃoː|ʃiː|huː|muː|fuː|ʃoː|deː|høː|ɡiː|huː|poː|ʃiː|ryː|koː|foː|fuː|huː|ʃoː|koː|deː|ʃiː|høː|foː|muː|ɡiː|ryː|poː|huː|ryː|høː|fuː|poː|deː|ɡiː|muː|koː|ʃiː|ʃoː|foː|deː|fuː|høː|huː|koː|ryː|muː|ʃiː|foː|poː|ʃoː|ɡiː|ʃiː|deː|poː|muː|ryː|ɡiː|foː|høː|koː|ʃoː|fuː|huː|høː|ɡiː|poː|ryː|muː|fuː|foː|huː|ʃoː|koː|deː|ʃiː|poː|ɡiː|ryː|fuː|foː|muː|huː|ʃiː|ʃoː|høː|deː|koː|foː|huː|fuː|deː|muː|høː|ʃoː|poː|ʃiː|ɡiː|koː|ryː|ʃiː|foː|deː|ryː|ɡiː|fuː|poː|ʃoː|muː|koː|høː|huː\n",
      "TP mode: random\n",
      "Lexicon: ɡiːʃoːmuː|foːhuːdeː|ryːkoːfuː|ʃiːpoːhøː\n",
      "Feature PRIs: {'phon_1_son': 0.06666666666666667, 'phon_1_back': 0.03508771929824561, 'phon_1_hi': 0.03508771929824561, 'phon_1_lab': 0.08947368421052632, 'phon_1_cor': 0.07719298245614035, 'phon_1_cont': 0.021052631578947368, 'phon_1_lat': 0.0, 'phon_1_nas': 0.0, 'phon_1_voi': 0.06842105263157895, 'phon_2_back': 0.021052631578947368, 'phon_2_hi': 0.043859649122807015, 'phon_2_lo': 0.0, 'phon_2_lab': 0.08596491228070176, 'phon_2_tense': 0.0, 'phon_2_long': 0.0}\n",
      "\n",
      "Total number of generated streams:  6\n"
     ]
    }
   ],
   "source": [
    "from arc import make_streams, make_lexicons, load_words\n",
    "\n",
    "def print_stream_info(stream):\n",
    "    print(\"Stream:\", \"|\".join([syll.id for syll in stream]))\n",
    "    print(\"TP mode:\", stream.info[\"stream_tp_mode\"])\n",
    "    print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "    print(\"Feature PRIs:\", stream.info[\"rhythmicity_indexes\"])\n",
    "    print(\"\")\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=20, n_words=4, control_features=True)\n",
    "streams = make_streams(lexicons, max_rhythmicity=0.1, require_all_tp_modes=True)\n",
    "print(\"\")\n",
    "\n",
    "print_stream_info(streams[0])\n",
    "\n",
    "print(\"Total number of generated streams: \", len(streams))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
