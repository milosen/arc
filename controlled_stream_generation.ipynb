{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Tutorial: Controlled Stream Generation\n",
    "\n",
    "We will generate words and a Lexicon with minimal feature overlap between the words. Next, we introduce the 3 main ways to generate random streams, depending on how the transition probabilities (TPs) of their syllables are structured: word-structured TPs, fully random (uniform) TPs, and position-controlled TPs.\n",
    "\n",
    "First, we generate/reload the words register (see arc types tutorial).\n",
    "\n",
    "## Syllable and Word Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75e7ba-dc5a-4e87-923a-c9423d498d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import load_phonemes, make_syllables, make_words\n",
    "\n",
    "print(\"Load phonemes...\")\n",
    "phonemes = load_phonemes()\n",
    "print(phonemes)\n",
    "\n",
    "print(\"Make syllables...\")\n",
    "syllables = make_syllables(phonemes, phoneme_pattern=\"cV\", unigram_control=True, language_control=True, language_alpha=0.05)\n",
    "print(syllables)\n",
    "\n",
    "print(\"Make words...\")\n",
    "words = make_words(syllables)\n",
    "print(words)\n",
    "\n",
    "print(\"Save words ...\")\n",
    "words.save(\"words.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413261-d35b-4c93-b6da-ff39cc51805b",
   "metadata": {},
   "source": [
    "## Lexicon Generation\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. Let's start with 4 words each. \n",
    "\n",
    "By default, the function will generate 5 `Lexicon`s max. Let's generate 2 and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_lexicons, load_words\n",
    "\n",
    "words = load_words(\"words.json\")\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=2, n_words=4, control_features=True)\n",
    "print(\"\")\n",
    "\n",
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"cumulative_feature_repetitiveness:\", lexicon.info[\"cumulative_feature_repetitiveness\"])\n",
    "    print(\"max_pairwise_feature_repetitiveness:\", lexicon.info[\"max_pairwise_feature_repetitiveness\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative feature repetitiveness will be generated first, starting at zero. This means words will be joined into a lexicon if the features of all word pairs in the lexicon have no overlap. If it is not possible to generate the requested number Lexicons with the zero overlap, the allowed overlap will be incremented untill all lexicons are collecteed, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "## Stream Generation\n",
    "\n",
    "We want to generate a complete set of compatible lexicons for our study, i.e. to generate a compatible set of streams for testing statistical learning hypotheses. If `streams` is empty, try increasing the allowed maximum rythmicity.\n",
    "\n",
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization/TP-structure mode has been used.\n",
    "\n",
    "The function `make_streams` will try to generate one stream for each lexicon and TP mode, and will discard those that do not meet the max_rhythmicity requirement. By default, all streams from a lexicon will be discarded if the lexicon can't generate streams for all requested TP modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea9483-fdad-470c-9caf-abff48b46a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream_info(stream):\n",
    "    print(\"Stream:\", \"|\".join([syll.id for syll in stream]))\n",
    "    print(\"TP mode:\", stream.info[\"stream_tp_mode\"])\n",
    "    print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "    print(\"Feature PRIs:\", stream.info[\"rhythmicity_indexes\"])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82199da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_streams, make_lexicons, load_words\n",
    "\n",
    "words = load_words(\"words.json\")\n",
    "lexicons = make_lexicons(words, n_lexicons=20, n_words=4, control_features=True)\n",
    "streams = make_streams(lexicons, max_rhythmicity=0.1, require_all_tp_modes=True)\n",
    "print(\"\")\n",
    "\n",
    "for stream in streams:\n",
    "    print_stream_info(stream)\n",
    "\n",
    "print(\"Num Streams: \", len(streams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2debc-1374-4624-ade1-60d9a051995e",
   "metadata": {},
   "source": [
    "# Compare Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39428d",
   "metadata": {},
   "source": [
    "We compare 3 ways to generate lexicons and streams: 1) ARC (ours), 2) Lexicons with randomly shuffled syllables, and 3) Reference lexicons from the literature. Each mode takes approx. 3min to compute on a MacBook Air M1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f78b01-78c8-4ba3-8844-c292cbe5b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LEXICONS = 21  # number of lexicons per TP mode\n",
    "N_WORDS_PER_LEXICON = 4  \n",
    "N_REPS = 5  # how often to randomize the lexicon until the stream is full, \n",
    "            # i.e. long will the streams be in lexicon lengths N_REPS*N_WORDS_PER_LEXICON = n syllables in the stream\n",
    "N_STREAMS_PER_INPUT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764ef8b",
   "metadata": {},
   "source": [
    "### ARC Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337f24-4b48-4598-ad47-00714b74921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc import make_streams\n",
    "from arc import make_lexicons, Register, load_phonemes, load_words\n",
    "\n",
    "print(\"Load words...\")\n",
    "words = load_words(\"words.json\")\n",
    "\n",
    "print(\"Make lexicons...\")\n",
    "controlled_lexicons = make_lexicons(words, n_lexicons=N_LEXICONS, n_words=N_WORDS_PER_LEXICON, control_features=True)\n",
    "for lexicon in controlled_lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"Info:\", lexicon.info)\n",
    "    print(\"\")\n",
    "\n",
    "controlled_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        controlled_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        controlled_streams.append(stream)\n",
    "\n",
    "for stream in controlled_streams:\n",
    "    print_stream_info(stream)\n",
    "\n",
    "len(controlled_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00475fed-bf37-4821-87a7-6962ef72145d",
   "metadata": {},
   "source": [
    "### Random / uncontrolled lexicons (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16a898-0eeb-4519-ad2e-ad625da4b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_lexicons = make_lexicons(words, n_lexicons=N_LEXICONS, n_words=N_WORDS_PER_LEXICON, control_features=False)\n",
    "for lexicon in random_lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"Info:\", lexicon.info)\n",
    "    print(\"\")\n",
    "\n",
    "random_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        random_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        random_streams.append(stream)\n",
    "for stream in random_streams:\n",
    "    print_stream_info(stream)\n",
    "\n",
    "len(random_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc5056",
   "metadata": {},
   "source": [
    "### Reference lexicons from the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2490522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from arc.core.syllable import LABELS_C, LABELS_V, syllable_from_phonemes\n",
    "from arc.core.word import Word, word_overlap_matrix\n",
    "\n",
    "phoneme_register = load_phonemes()\n",
    "\n",
    "syll_feature_labels = [LABELS_C, LABELS_V]\n",
    "syllable_type = \"cv\"\n",
    "\n",
    "lexicons = [[['pi', 'ɾu', 'ta'],\n",
    "  ['ba', 'ɡo', 'li'],\n",
    "  ['to', 'ku', 'da'],\n",
    "  ['ɡu', 'ki', 'bo']],\n",
    " [['pa', 'be', 'la'],\n",
    "  ['di', 'ne', 'ka'],\n",
    "  ['lu', 'fa', 'ri'],\n",
    "  ['xi', 'so', 'du']],\n",
    " [['ma', 'xu', 'pe'],\n",
    "  ['xe', 'ro', 'ɡa'],\n",
    "  ['de', 'mu', 'si'],\n",
    "  ['fo', 'le', 'ti']],\n",
    " [['pu', 'ke', 'mi'],\n",
    "  ['ra', 'fi', 'nu'],\n",
    "  ['bi', 'na', 'po'],\n",
    "  ['me', 'do', 'xi']],\n",
    " [['no', 'ni', 'xe'],\n",
    "  ['bu', 'lo', 'te'],\n",
    "  ['re', 'mo', 'fu'],\n",
    "  ['ko', 'tu', 'sa']],\n",
    " [['mi', 'lo', 'de'],\n",
    "  ['da', 'le', 'bu'],\n",
    "  ['no', 'ru', 'pa'],\n",
    "  ['ka', 'te', 'xi']],\n",
    " [['ne', 'do', 'li'],\n",
    "  ['ri', 'fo', 'nu'],\n",
    "  ['ba', 'to', 'ɡu'],\n",
    "  ['ki', 'ra', 'pu']],\n",
    " [['ɡo', 'na', 'be'],\n",
    "  ['mu', 'di', 'la'],\n",
    "  ['ro', 'ni', 'xe'],\n",
    "  ['pi', 'ku', 'sa']],\n",
    " [['fu', 'bi', 're'],\n",
    "  ['xe', 'tu', 'si'],\n",
    "  ['ta', 'fi', 'ko'],\n",
    "  ['ke', 'ma', 'po']],\n",
    " [['ti', 'fa', 'xu'],\n",
    "  ['so', 'du', 'xi'],\n",
    "  ['me', 'lu', 'bo'],\n",
    "  ['ɡa', 'ni', 'pe']],\n",
    " [['mi', 'po', 'la'],\n",
    "  ['za', 'bɛ', 'tu'],\n",
    "  ['ʁo', 'ki', 'sɛ'],\n",
    "  ['nu', 'ɡa', 'di']],\n",
    " [['dɛ', 'mʊ', 'ri'],\n",
    "  ['sɛ', 'ni', 'ɡɛ'],\n",
    "  ['ræ', 'ku', 'səʊ'],\n",
    "  ['pi', 'lɛ', 'ru']],\n",
    " [['ki', 'fəʊ', 'bu'],\n",
    "  ['lu', 'fɑ', 'ɡi'],\n",
    "  ['pæ', 'beɪ', 'lɑ'],\n",
    "  ['tɑ', 'ɡəʊ', 'fʊ']],\n",
    " [['bi', 'du', 'pɛ'],\n",
    "  ['məʊ', 'bɑ', 'li'],\n",
    "  ['rɛ', 'ɡæ', 'tʊ'],\n",
    "  ['sæ', 'tɛ', 'kəʊ']],\n",
    " [['bəʊ', 'dɑ', 'mɛ'],\n",
    "  ['fi', 'nəʊ', 'pɑ'],\n",
    "  ['ɡʊ', 'rɑ', 'təʊ'],\n",
    "  ['ləʊ', 'kæ', 'neɪ']],\n",
    " [['fɛ', 'si', 'nɑ'],\n",
    "  ['kɛ', 'su', 'dəʊ'],\n",
    "  ['mæ', 'pʊ', 'di'],\n",
    "  ['ti', 'mi', 'nu']],\n",
    " [['tu', 'pi', 'ɹoʊ'],\n",
    "  ['ɡoʊ', 'la', 'bu'],\n",
    "  ['pa', 'doʊ', 'ti'],\n",
    "  ['bi', 'da', 'ku']],\n",
    " [['meɪ', 'lu', 'ɡi'],\n",
    "  ['ɹa', 'fi', 'nu'],\n",
    "  ['pu', 'keɪ', 'mi'],\n",
    "  ['toʊ', 'na', 'poʊ']],\n",
    " [['ɡoʊ', 'la', 'tu'],\n",
    "  ['da', 'ɹoʊ', 'pi'],\n",
    "  ['ti', 'bu', 'doʊ'],\n",
    "  ['pa', 'bi', 'ku']],\n",
    " [['poʊ', 'fi', 'mu'],\n",
    "  ['noʊ', 'vu', 'ka'],\n",
    "  ['vi', 'koʊ', 'ɡa'],\n",
    "  ['ba', 'fu', 'ɡi']],\n",
    " [['ma', 'nu', 'toʊ'],\n",
    "  ['ni', 'moʊ', 'lu'],\n",
    "  ['voʊ', 'ɹi', 'fa'],\n",
    "  ['li', 'du', 'ɹa']]]\n",
    "\n",
    "def to_phoneme(phoneme):\n",
    "    return phoneme\n",
    "\n",
    "def to_syllable(syllable):\n",
    "    if len(syllable) == 3:\n",
    "        syllable_obj = syllable_from_phonemes(phonemes, syllable[:3], syll_feature_labels)\n",
    "        syllable_obj.id = syllable\n",
    "        return syllable_obj\n",
    "    return syllable_from_phonemes(phonemes, syllable[:3], syll_feature_labels)\n",
    "\n",
    "def to_word(word):\n",
    "    syllables_list = list(map(to_syllable, word))\n",
    "    word_id = \"\".join(s.id for s in syllables_list)\n",
    "    word_features = list(list(tup) for tup in zip(*[s.info[\"binary_features\"] for s in syllables_list]))\n",
    "    return Word(id=word_id, info={\"binary_features\": word_features}, syllables=syllables_list)\n",
    "\n",
    "def to_lexicon(lexicon):\n",
    "    word_objs_list = list(map(to_word, lexicon))\n",
    "    lexicon = Register({w.id:  w for w in word_objs_list})\n",
    "    lexicon.info.update({\"syllable_feature_labels\": [LABELS_C, LABELS_V],  \"syllable_type\": syllable_type})\n",
    "    overlap = word_overlap_matrix(lexicon)\n",
    "    lexicon.info[\"cumulative_feature_repetitiveness\"] = np.triu(overlap,1).sum()\n",
    "    lexicon.info[\"max_pairwise_feature_repetitiveness\"] = np.triu(overlap,1).max()\n",
    "    return lexicon\n",
    "\n",
    "ref_lexicons = list(map(to_lexicon, lexicons))\n",
    "\n",
    "for lex in ref_lexicons:\n",
    "    print(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        ref_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        ref_streams.append(stream)\n",
    "\n",
    "for stream in ref_streams:\n",
    "    print_stream_info(stream)\n",
    "\n",
    "len(ref_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8e397-1ac5-4dac-84bb-106eaae80e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\"Control\": [], \"Lexicon\": [], \"Feature\": [], \"PRI\": [], \"Stream TP mode\": [], \"Stream\": []}\n",
    "\n",
    "mode_to_mode = {  # TP-random position-random; TP-random position-fixed and TP-structured\n",
    "    \"random\": \"TP-random position-random\",\n",
    "    \"word_structured\": \"TP-structured\",\n",
    "    \"position_controlled\": \"TP-random position-fixed\"\n",
    "}\n",
    "\n",
    "for control, streams in [(\"Controlled lexicons (ARC)\", controlled_streams), (\"Reference lexicons (Literature)\", ref_streams), (\"Random lexicons (Baseline)\", random_streams)]:\n",
    "    for stream in streams:\n",
    "        for k, v in stream.info[\"rhythmicity_indexes\"].items():\n",
    "            data[\"Feature\"].append(k)\n",
    "            data[\"PRI\"].append(v)\n",
    "            data[\"Control\"].append(control)\n",
    "            data[\"Lexicon\"].append(str(stream.info[\"lexicon\"]))\n",
    "            data[\"Stream TP mode\"].append(mode_to_mode[stream.info[\"stream_tp_mode\"]])\n",
    "            data[\"Stream\"].append(\"|\".join(syll.id for syll in stream))\n",
    "        data[\"Feature\"].append(\"max\")\n",
    "        data[\"PRI\"].append(max(stream.info[\"rhythmicity_indexes\"].values()))\n",
    "        data[\"Control\"].append(control)\n",
    "        data[\"Lexicon\"].append(str(stream.info[\"lexicon\"]))\n",
    "        data[\"Stream TP mode\"].append(mode_to_mode[stream.info[\"stream_tp_mode\"]])\n",
    "        data[\"Stream\"].append(\"|\".join(syll.id for syll in stream))\n",
    "\n",
    "df = pd.DataFrame(data).sort_values([\"Control\", \"Lexicon\", \"Stream TP mode\"]).reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"docs/full_dataset.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lexicons = df[[\"Control\", \"Lexicon\"]].drop_duplicates().reset_index(drop=True)\n",
    "df_lexicons.to_csv(\"all_lexicons.csv\")\n",
    "df_lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ed81c",
   "metadata": {},
   "source": [
    "## Plots & Stats from the Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"axes.titlesize\"] = 14.\n",
    "mpl.rcParams[\"axes.labelsize\"] = 12.\n",
    "mpl.rcParams[\"axes.titleweight\"] = \"bold\"\n",
    "mpl.rcParams[\"axes.labelweight\"] = \"normal\"\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "\n",
    "cm = 1/2.54  # centimeters in inches\n",
    "for key in mpl.rcParams:\n",
    "    if \"axes\" in key:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "from pingouin import ttest\n",
    "\n",
    "tp_modes_pretty = [\"TP-random position-random\", \"TP-random position-fixed\", \"TP-structured\"]\n",
    "dfs = []\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Controlled lexicons (ARC)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Reference lexicons (Literature)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"controlled vs. reference {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Controlled lexicons (ARC)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Random lexicons (Baseline)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"controlled vs. random baseline {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Reference lexicons (Literature)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Random lexicons (Baseline)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"reference vs. random baseline {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "ttest_df = pd.concat(dfs)\n",
    "\n",
    "display(ttest_df)\n",
    "\n",
    "ttest_df.to_csv(\"ttest_results.csv\")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(17*cm, 10*cm))\n",
    "sns.boxplot(df[df[\"Feature\"] == \"max\"], x=\"Stream TP mode\", y=\"PRI\", hue=\"Control\", order=tp_modes_pretty, gap=0.2)\n",
    "plt.title(\"Maximum PRI Across Features\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels)\n",
    "\n",
    "plt.savefig(\"lexicon_pris_summary.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86faf394-48b3-4b77-a69c-3595116e2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axs = plt.subplots(3, 1, figsize=(17*cm, 20*cm), layout=\"tight\", sharex=True)\n",
    "labels = ['Controlled\\nlexicons (ARC)',\n",
    " 'Reference\\nlexicons (Literature)',\n",
    " 'Random\\nlexicons (Baseline)']\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    sns.boxplot(df[df[\"Stream TP mode\"] == tp_mode], x=\"Feature\", y=\"PRI\", hue=\"Control\", ax=axs[i], fliersize=1, gap=0.3)\n",
    "    axs[i].set_title(f\"Stream TP mode: {tp_mode}\", weight=\"normal\", size=12)\n",
    "    if i == 0:\n",
    "        axs[i].set(ylim=(-0.02, 0.4))\n",
    "        # axs[i].legend(labels)\n",
    "        # sns.move_legend(axs[i], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "        sns.move_legend(axs[i], \"upper left\")\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(handles=handles, labels=labels)\n",
    "        \n",
    "    else:\n",
    "        axs[i].legend([],[], frameon=False)\n",
    "locs, labls = plt.xticks()\n",
    "plt.xticks(locs, labls, rotation=60)\n",
    "plt.suptitle(f\"PRIs Accross Features\", weight=\"bold\", size=14)\n",
    "plt.savefig(\"lexicon_pri.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56b23e-5dd5-47be-91ed-b7a345efc35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91438726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = df[(df[\"Feature\"] == \"max\") & (df[\"Control\"] == \"Controlled lexicons (ARC)\") & (df[\"Stream TP mode\"] == mode_to_mode[\"word_structured\"])]\n",
    "df_best = df_max[df_max.PRI == df_max.PRI.min()]\n",
    "display(df_best)\n",
    "best_lexicon = list(df_best[\"Lexicon\"])[0]\n",
    "\n",
    "for stream in controlled_streams:\n",
    "    if str(stream.info[\"lexicon\"]) == best_lexicon:\n",
    "        print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "        print(\"Stream:\", \"|\".join(syll.id for syll in stream))\n",
    "        print(\"Stream TP mode:\", mode_to_mode[stream.info[\"stream_tp_mode\"]])\n",
    "        print(stream.info[\"rhythmicity_indexes\"])\n",
    "        print(\"Max PRI:\", max(stream.info[\"rhythmicity_indexes\"].values()))\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57064562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae02988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7dad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
